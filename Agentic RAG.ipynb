{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"modules\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/utils.py\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key(key_name=\"OPENROUTER_API_KEY\"):\n",
    "    \"\"\"\n",
    "    Get API key from environment variables\n",
    "\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(key_name)\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(f\"Invalid API key: {key_name} not found in environment variables\")\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "def initialize_llm(model_name=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "                  temperature=0.4,\n",
    "                  use_streaming=True):\n",
    "    \"\"\"\n",
    "    Initialize LLM\n",
    "\n",
    "    \"\"\"\n",
    "    api_key = get_api_key()\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "        streaming=use_streaming,\n",
    "        callbacks=callbacks,\n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=\"https://openrouter.ai/api/v1\"\n",
    "    )\n",
    "    \n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/retriever.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/retriever.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import (\n",
    "    EmbeddingsFilter, \n",
    "    LLMChainFilter, \n",
    "    LLMChainExtractor, \n",
    "    DocumentCompressorPipeline\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from utils import initialize_llm\n",
    "\n",
    "def load_documents(docs_dir: str = \"books\") -> List:\n",
    "    \"\"\"\n",
    "    Load documents from directory\n",
    "    \n",
    "    \"\"\"\n",
    "    if not os.path.exists(docs_dir):\n",
    "        raise ValueError(f\"The specified directory {docs_dir} does not exist. Please enter a valid directory\")\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        docs_dir,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def create_vectorstore(documents, embeddings=None, store_type: str = \"faiss\", persist_directory: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create vector store from documents\n",
    "    \n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split the documents into {len(chunks)} chunks\")\n",
    "    \n",
    "    if embeddings is None:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "        \n",
    "    # Create Vector Store\n",
    "    if store_type.lower() == \"faiss\":\n",
    "        vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "        if persist_directory:\n",
    "            vector_store.save_local(persist_directory)\n",
    "    elif store_type.lower() == \"chroma\":\n",
    "        if persist_directory:            \n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            vector_store.persist()\n",
    "        else:\n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=embeddings\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown vector store type {store_type}\")\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "def initialize_retriever(docs_dir: str = \"books\", \n",
    "                         store_type: str = \"faiss\", \n",
    "                         persist_directory: Optional[str] = \"vector\", \n",
    "                         similarity_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Initialize retriever\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if vector store exists\n",
    "    vector_store = None\n",
    "    if persist_directory and os.path.exists(persist_directory):\n",
    "        # Load existing vector store\n",
    "        print(f\"Loading vector store from {persist_directory}\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "        if store_type.lower() == \"faiss\":\n",
    "            vector_store = FAISS.load_local(persist_directory, embeddings, allow_dangerous_deserialization=True)\n",
    "        elif store_type.lower() == \"chroma\":\n",
    "            vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "            \n",
    "    # If vector store doesn't exist, create it\n",
    "    if vector_store is None:\n",
    "        documents = load_documents(docs_dir)\n",
    "        if not documents:\n",
    "            print(\"No documents in the directory\")\n",
    "            return None\n",
    "        \n",
    "        # Create Vector Store\n",
    "        vector_store = create_vectorstore(\n",
    "            documents,\n",
    "            store_type=store_type,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    # Base Retriever\n",
    "    base_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "    \n",
    "    # Create the embeddings and the embeddings filter\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "    embeddings_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=similarity_threshold\n",
    "    )\n",
    "\n",
    "    # Create LLMChain Extractor to extract the relevant documents\n",
    "    llm = initialize_llm()\n",
    "    llm_extractor = LLMChainExtractor.from_llm(llm=llm)\n",
    "\n",
    "    # Create a pipeline of compressors\n",
    "    compression_pipeline = DocumentCompressorPipeline(\n",
    "        transformers=[embeddings_filter, llm_extractor]\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compression_pipeline,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
