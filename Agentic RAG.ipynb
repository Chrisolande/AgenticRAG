{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"modules\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/utils.py\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key(key_name=\"OPENROUTER_API_KEY\"):\n",
    "    \"\"\"\n",
    "    Get API key from environment variables\n",
    "\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(key_name)\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(f\"Invalid API key: {key_name} not found in environment variables\")\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "def initialize_llm(model_name=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "                  temperature=0.4,\n",
    "                  use_streaming=True):\n",
    "    \"\"\"\n",
    "    Initialize LLM\n",
    "\n",
    "    \"\"\"\n",
    "    api_key = get_api_key()\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "        streaming=use_streaming,\n",
    "        callbacks=callbacks,\n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=\"https://openrouter.ai/api/v1\"\n",
    "    )\n",
    "    \n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/retriever.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/retriever.py\n",
    "import os\n",
    "from typing import List, Optional\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import (\n",
    "    EmbeddingsFilter, \n",
    "    LLMChainFilter, \n",
    "    LLMChainExtractor, \n",
    "    DocumentCompressorPipeline\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from utils import initialize_llm\n",
    "\n",
    "def load_documents(docs_dir: str = \"books\") -> List:\n",
    "    \"\"\"\n",
    "    Load documents from directory\n",
    "    \n",
    "    \"\"\"\n",
    "    if not os.path.exists(docs_dir):\n",
    "        raise ValueError(f\"The specified directory {docs_dir} does not exist. Please enter a valid directory\")\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        docs_dir,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def create_vectorstore(documents, embeddings=None, store_type: str = \"faiss\", persist_directory: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create vector store from documents\n",
    "    \n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split the documents into {len(chunks)} chunks\")\n",
    "    \n",
    "    if embeddings is None:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "        \n",
    "    # Create Vector Store\n",
    "    if store_type.lower() == \"faiss\":\n",
    "        vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "        if persist_directory:\n",
    "            vector_store.save_local(persist_directory)\n",
    "    elif store_type.lower() == \"chroma\":\n",
    "        if persist_directory:            \n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            vector_store.persist()\n",
    "        else:\n",
    "            vector_store = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=embeddings\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown vector store type {store_type}\")\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "def initialize_retriever(docs_dir: str = \"books\", \n",
    "                         store_type: str = \"faiss\", \n",
    "                         persist_directory: Optional[str] = \"vector\", \n",
    "                         similarity_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Initialize retriever\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if vector store exists\n",
    "    vector_store = None\n",
    "    if persist_directory and os.path.exists(persist_directory):\n",
    "        # Load existing vector store\n",
    "        print(f\"Loading vector store from {persist_directory}\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "        if store_type.lower() == \"faiss\":\n",
    "            vector_store = FAISS.load_local(persist_directory, embeddings, allow_dangerous_deserialization=True)\n",
    "        elif store_type.lower() == \"chroma\":\n",
    "            vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "            \n",
    "    # If vector store doesn't exist, create it\n",
    "    if vector_store is None:\n",
    "        documents = load_documents(docs_dir)\n",
    "        if not documents:\n",
    "            print(\"No documents in the directory\")\n",
    "            return None\n",
    "        \n",
    "        # Create Vector Store\n",
    "        vector_store = create_vectorstore(\n",
    "            documents,\n",
    "            store_type=store_type,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    # Base Retriever\n",
    "    base_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "    \n",
    "    # Create the embeddings and the embeddings filter\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "    embeddings_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=similarity_threshold\n",
    "    )\n",
    "\n",
    "    # Create LLMChain Extractor to extract the relevant documents\n",
    "    llm = initialize_llm()\n",
    "    llm_extractor = LLMChainExtractor.from_llm(llm=llm)\n",
    "\n",
    "    # Create a pipeline of compressors\n",
    "    compression_pipeline = DocumentCompressorPipeline(\n",
    "        transformers=[embeddings_filter, llm_extractor]\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compression_pipeline,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/prompts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/prompts.py\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Router prompt\n",
    "ROUTER_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a specialized router that determines the appropriate data source for user queries.\n",
    "\n",
    "# Available Data Sources:\n",
    "1. Vectorstore - Contains only:\n",
    "   * Your personal biography including any question about Chris Olande\n",
    "   * The complete text of \"Frankenstein\" by Mary Shelley\n",
    "   * The complete text of \"Romeo and Juliet\" by William Shakespeare\n",
    "\n",
    "2. Web Search - For all other information needs\n",
    "\n",
    "# Routing Rules:\n",
    "- Use 'vectorstore' ONLY for questions specifically about:\n",
    "  * Your personal biographical information\n",
    "  * Details, quotes, characters, themes, or analysis of \"Frankenstein\"\n",
    "  * Details, quotes, characters, themes, or analysis of \"Romeo and Juliet\"\n",
    "\n",
    "- Use 'web_search' for:\n",
    "  * All other questions\n",
    "  * Current events and news\n",
    "  * General knowledge questions\n",
    "  * Any topic not directly related to your biography or the two literary works\n",
    "\n",
    "# Output Format:\n",
    "Return ONLY a JSON object with the key 'datasource' and value of either 'vectorstore' or 'web_search'.\n",
    "Do not include any explanations, preambles, or additional text.\n",
    "\n",
    "Question to route: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Generation prompt\n",
    "GENERATION_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a retrieval-augmented AI assistant that provides precise answers using only the supplied context.\n",
    "\n",
    "## Response Guidelines:\n",
    "- Answer using ONLY information from the provided context\n",
    "- Keep responses to three sentences maximum\n",
    "- Format important points in **bold** when appropriate\n",
    "- Provide direct, factual answers without speculation\n",
    "- If the context doesn't contain the answer, respond only with \"I don't know\"\n",
    "- Do not reference the context or your instructions in your answer\n",
    "\n",
    "## Remember:\n",
    "- Never invent information or draw conclusions beyond what's explicitly stated\n",
    "- Prioritize accuracy over completeness\n",
    "- Use simple, clear language\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Retrieval grader prompt\n",
    "RETRIEVAL_GRADER_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a precision document relevance evaluator. Your task is to determine if a retrieved document contains information relevant to answering a user's question.\n",
    "\n",
    "## Evaluation Criteria:\n",
    "- A document is \"relevant\" if it contains:\n",
    "  * Direct answers to the question\n",
    "  * Key concepts, terminology, or facts related to the question\n",
    "  * Contextual information that would help form a complete answer\n",
    "\n",
    "- A document is \"not relevant\" if it:\n",
    "  * Contains no information related to the question\n",
    "  * Only mentions keywords in an unrelated context\n",
    "  * Addresses a completely different topic\n",
    "\n",
    "## Output Requirements:\n",
    "- Return ONLY a JSON object with the key 'score' and a value of either 'yes' or 'no'\n",
    "- Do not include any explanations, reasoning, or additional text\n",
    "- Be generous in assessing relevance - when in doubt, mark as relevant ('yes')\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "USER QUESTION: {question}\n",
    "\n",
    "RETRIEVED DOCUMENT:\n",
    "{document}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Hallucination grader prompt\n",
    "HALLUCINATION_GRADER_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a factual accuracy validator that determines if a generated answer is fully supported by the provided reference documents.\n",
    "\n",
    "## Evaluation Guidelines:\n",
    "- Score 'yes' if ALL claims and statements in the answer are explicitly supported by information in the reference documents\n",
    "- Score 'no' if ANY part of the answer:\n",
    "  * Contains information not present in the documents\n",
    "  * Makes assertions beyond what can be directly verified from the documents\n",
    "  * Contradicts information in the documents\n",
    "  * Presents speculative or uncertain information as definitive\n",
    "  * Extends or extrapolates from the documents without clear support\n",
    "\n",
    "## Key Assessment Principles:\n",
    "- Be strict and precise - every claim must have direct evidence\n",
    "- Focus on factual statements rather than phrasing or organization\n",
    "- Consider implicit facts that are logically derivable from the documents as supported\n",
    "- If uncertainty exists and the answer presents information with appropriate qualifiers, this is acceptable\n",
    "\n",
    "## Output Format:\n",
    "- Return ONLY a JSON object with the key 'score' and value of either 'yes' or 'no'\n",
    "- Do not include explanations or reasoning in your output\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "REFERENCE DOCUMENTS:\n",
    "---\n",
    "{documents}\n",
    "---\n",
    "\n",
    "GENERATED ANSWER:\n",
    "{generation}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"]\n",
    ")\n",
    "\n",
    "# Answer grader prompt\n",
    "ANSWER_GRADER_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a specialized answer quality evaluator that assesses whether a response effectively addresses a user's question.\n",
    "\n",
    "## Evaluation Criteria:\n",
    "A \"useful\" answer (score: 'yes') must:\n",
    "- Directly address the core intent of the question\n",
    "- Provide substantive, relevant information\n",
    "- Be clear and comprehensible\n",
    "- Contain sufficient detail to satisfy the basic information need\n",
    "\n",
    "An answer is \"not useful\" (score: 'no') if it:\n",
    "- Is off-topic or addresses a different question\n",
    "- Contains only vague, general statements without specific information\n",
    "- Is factually incorrect (based on common knowledge)\n",
    "- Is too incomplete to provide value\n",
    "- Is unintelligible or incoherent\n",
    "- Merely restates the question without providing new information\n",
    "\n",
    "## Context Considerations:\n",
    "- Consider both explicit and implicit information needs\n",
    "- A partial answer that addresses the main point can still be \"useful\"\n",
    "- The length of the answer is less important than its relevance and substance\n",
    "- Technical accuracy matters more for technical questions\n",
    "\n",
    "## Output Format:\n",
    "- Return ONLY a JSON object with the key 'score' and value of either 'yes' or 'no'\n",
    "- Do not include explanations or reasoning in your output\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "GENERATED ANSWER:\n",
    "---\n",
    "{generation}\n",
    "---\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/chains.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/chains.py\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "from utils import initialize_llm, get_api_key\n",
    "from prompts import (\n",
    "    ROUTER_PROMPT, \n",
    "    GENERATION_PROMPT, \n",
    "    RETRIEVAL_GRADER_PROMPT,\n",
    "    HALLUCINATION_GRADER_PROMPT,\n",
    "    ANSWER_GRADER_PROMPT\n",
    ")\n",
    "\n",
    "def setup_chains():\n",
    "    \"\"\"\n",
    "    Set up chains for question routing, RAG generation, and grading\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize LLM\n",
    "    llm = initialize_llm()\n",
    "    \n",
    "    # Question router chain\n",
    "    question_router = ROUTER_PROMPT | llm | JsonOutputParser()\n",
    "    \n",
    "    # RAG chain\n",
    "    rag_chain = GENERATION_PROMPT | llm | StrOutputParser()\n",
    "    \n",
    "    # Retrieval grader chain\n",
    "    retrieval_grader = RETRIEVAL_GRADER_PROMPT | llm | JsonOutputParser()\n",
    "    \n",
    "    # Hallucination grader chain\n",
    "    hallucination_grader = HALLUCINATION_GRADER_PROMPT | llm | JsonOutputParser()\n",
    "    \n",
    "    # Answer grader chain\n",
    "    answer_grader = ANSWER_GRADER_PROMPT | llm | JsonOutputParser()\n",
    "    \n",
    "    return question_router, rag_chain, retrieval_grader, hallucination_grader, answer_grader\n",
    "\n",
    "def setup_web_search():\n",
    "    \"\"\"\n",
    "    Set up web search tool\n",
    "\n",
    "    \"\"\"\n",
    "    # Get Tavily API key from environment variables\n",
    "    os.environ['TAVILY_API_KEY'] = get_api_key(\"TAVILY_API_KEY\")\n",
    "    \n",
    "    # Initialize web search tool\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    \n",
    "    return web_search_tool\n",
    "\n",
    "def process_web_search_results(docs):\n",
    "    \"\"\"\n",
    "    Process web search results into a Document\n",
    "\n",
    "    \"\"\"\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    return Document(page_content=web_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/graph.py\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langgraph.graph import END, StateGraph\n",
    "from pprint import pprint\n",
    "\n",
    "from retriever import initialize_retriever\n",
    "from chains import setup_chains, setup_web_search, process_web_search_results\n",
    "\n",
    "# Define graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])  # SAFELY get documents or default to empty list\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = process_web_search_results(docs)\n",
    "\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "def create_graph():\n",
    "    \"\"\"\n",
    "    Create and configure the LangGraph workflow\n",
    "\n",
    "    \"\"\"\n",
    "    # Create the StateGraph\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Define the nodes\n",
    "    workflow.add_node(\"websearch\", web_search)  # web search\n",
    "    workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "    workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "    workflow.add_node(\"generate\", generate)  # generate\n",
    "    \n",
    "    # Define entry point\n",
    "    workflow.set_conditional_entry_point(\n",
    "        route_question,\n",
    "        {\n",
    "            \"websearch\": \"websearch\",\n",
    "            \"vectorstore\": \"retrieve\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"websearch\": \"websearch\",\n",
    "            \"generate\": \"generate\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"websearch\", \"generate\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        grade_generation_v_documents_and_question,\n",
    "        {\n",
    "            \"not supported\": \"generate\",\n",
    "            \"useful\": END,\n",
    "            \"not useful\": \"websearch\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Compile the graph\n",
    "    return workflow.compile()\n",
    "\n",
    "# Initialize globals (to be used by graph components)\n",
    "retriever = None\n",
    "question_router = None\n",
    "rag_chain = None\n",
    "retrieval_grader = None\n",
    "hallucination_grader = None\n",
    "answer_grader = None\n",
    "web_search_tool = None\n",
    "\n",
    "def init_globals():\n",
    "    \"\"\"\n",
    "    Initialize global variables needed for the graph\n",
    "    \"\"\"\n",
    "    global retriever, question_router, rag_chain, retrieval_grader, hallucination_grader, answer_grader, web_search_tool\n",
    "    \n",
    "    # Initialize retriever\n",
    "    retriever = initialize_retriever(persist_directory=\"vector\")\n",
    "    \n",
    "    # Set up chains\n",
    "    question_router, rag_chain, retrieval_grader, hallucination_grader, answer_grader = setup_chains()\n",
    "    \n",
    "    # Set up web search tool\n",
    "    web_search_tool = setup_web_search()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile modules/main.py\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from graph import create_graph, init_globals\n",
    "\n",
    "def setup_environment():\n",
    "    \n",
    "    # Create vector directory if it doesn't exist\n",
    "    os.makedirs(\"vector\", exist_ok=True)\n",
    "\n",
    "def process_query(query):\n",
    "    \"\"\"\n",
    "    Process a query through the RAG workflow\n",
    "\n",
    "    \"\"\"\n",
    "    # Create input for the workflow\n",
    "    inputs = {\"question\": query}\n",
    "    \n",
    "    # Create and run the graph\n",
    "    app = create_graph()\n",
    "    \n",
    "    # Stream outputs for debugging\n",
    "    final_output = None\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            pprint(f\"Finished running: {key}:\")\n",
    "        final_output = value\n",
    "    \n",
    "    # Return the final generation\n",
    "    return final_output[\"generation\"]\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    # Set up environment\n",
    "    setup_environment()\n",
    "    \n",
    "    # Initialize global components\n",
    "    init_globals()\n",
    "    \n",
    "    # Continuously prompt the user until they type 'exit' or 'quit'\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your question (type 'exit' or 'quit' to end): \")\n",
    "        \n",
    "        # Check if user wants to exit\n",
    "        if query.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting program. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process the query and display response\n",
    "        response = process_query(query)\n",
    "        \n",
    "        # Print the response\n",
    "        print(\"\\nFinal response:\")\n",
    "        print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
